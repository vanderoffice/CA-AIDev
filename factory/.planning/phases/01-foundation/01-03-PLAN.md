---
phase: 01-foundation
plan: 03
type: execute
---

<objective>
Create knowledge document templates, decision tree JSON schema, and RAG table migration SQL.

Purpose: Standardize how knowledge documents are authored (consistent frontmatter, chunk-friendly structure) and migrate existing bot RAG tables to the `{schema}.document_chunks` naming convention.
Output: Knowledge TEMPLATE.md, decision-tree-schema.json, and migrate-rag-tables.sql ready for review and execution.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md

**Current RAG table state (discovered):**
| Bot | Current Location | Target |
|-----|-----------------|--------|
| WaterBot | `public.waterbot_documents` (n8n DB) | `waterbot.document_chunks` |
| BizBot | `public.bizbot_documents` (postgres DB) | `bizbot.document_chunks` |
| KiddoBot | `kiddobot.document_chunks` | Already correct — no migration |

**WaterBot current columns:** id, content, metadata (JSONB), embedding (vector 1536)
**BizBot current columns:** id, content, content_hash, embedding, source_file, source_section, chunk_index, topic, industries[], agencies[], cities[], counties[], chunk_level, verification_status, confidence, created_at, updated_at

**Standard column schema (from Plan 01-02):**
id, document_id, chunk_text, chunk_index, file_name, file_path, category, subcategory, section_title, char_count, content_hash, embedding, metadata (JSONB), created_at, updated_at

**Decision tree structure (from WaterBot/KiddoBot):**
- Nodes keyed by string ID ("start", "result_permit_needed", etc.)
- Question nodes: { type: "question", question, helpText, options: [{ label, next, icon }] }
- Result nodes: { type: "result", title, description, ragQuery?, links?: [{ label, url }] }

**Infrastructure notes:**
- VPS DB access: `ssh vps "docker exec -i supabase-db psql -U postgres -d postgres"`
- WaterBot data is in `n8n` database (NOT postgres) — need to check if migration covers cross-database or if it's same instance
- Migration must be reversible (CREATE ... IF NOT EXISTS, don't DROP originals until verified)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create knowledge document template and decision tree schema</name>
  <files>
    factory/templates/knowledge/TEMPLATE.md,
    factory/templates/knowledge/decision-tree-schema.json
  </files>
  <action>
    **TEMPLATE.md** — Standard knowledge document format:
    ```markdown
    ---
    title: "[Document Title]"
    domain: "[agency-or-domain-slug]"
    category: "[topic-category]"
    subcategory: "[specific-topic]"
    source_authority: "[authorizing body, e.g. State Water Resources Control Board]"
    source_urls:
      - "[primary source URL]"
    last_verified: "YYYY-MM-DD"
    ---

    # [Document Title]

    ## Overview
    [Brief description — what this document covers, who it's for, why it matters]

    ## [Topic Section 1]
    [Each H2 header becomes a chunk boundary during processing]

    ### [Subsection]
    [H3 content stays with its parent H2 chunk]

    ## [Topic Section 2]
    [Keep sections focused — aim for 500-1500 chars per H2 section]

    ## Key Requirements
    [Summarize actionable requirements, regulations, or procedures]

    ## Related Resources
    - [Link to related document or external resource](URL)
    - [Another related resource](URL)
    ```

    Include comments (HTML `<!-- -->`) explaining:
    - Why H2 = chunk boundary (this is what chunk-knowledge.js splits on)
    - Ideal section length (500-1500 chars, max 2000 before auto-split)
    - Frontmatter fields are injected into every chunk's metadata
    - H1 title is prefixed to every chunk for retrieval context

    **decision-tree-schema.json** — JSON Schema (draft-07) formalizing the decision tree structure:
    ```json
    {
      "$schema": "http://json-schema.org/draft-07/schema#",
      "title": "Decision Tree Schema",
      "description": "Standard schema for bot decision trees used by DecisionTreeView component",
      ...
    }
    ```

    The schema must validate:
    - Root has `meta` (name, version, description) and `nodes` objects
    - Every node has `type` ("question" or "result")
    - Question nodes require: question (string), options (array of {label, next, icon?})
    - Result nodes require: title, description. Optional: ragQuery, links[]
    - `next` values in options must reference existing node keys (use pattern, not $ref — keep it simple)
    - Must have a "start" node

    Derive the schema from actual WaterBot/KiddoBot decision tree JSON files — don't invent structure.
  </action>
  <verify>
    ```bash
    # Template exists with frontmatter
    head -15 factory/templates/knowledge/TEMPLATE.md

    # Schema is valid JSON
    python3 -c "import json; json.load(open('factory/templates/knowledge/decision-tree-schema.json')); print('Valid JSON ✓')"

    # Schema has required fields
    python3 -c "
    import json
    s = json.load(open('factory/templates/knowledge/decision-tree-schema.json'))
    assert 'properties' in s or 'definitions' in s, 'Missing schema structure'
    print(f'Schema title: {s.get(\"title\", \"N/A\")}')
    print('Schema structure valid ✓')
    "
    ```
  </verify>
  <done>TEMPLATE.md has YAML frontmatter with all 6 fields, H2 section structure with authoring guidance comments. decision-tree-schema.json validates question/result node types, requires "start" node, documents all fields from WaterBot/KiddoBot trees.</done>
</task>

<task type="auto">
  <name>Task 2: Create RAG table migration SQL</name>
  <files>factory/scripts/migrate-rag-tables.sql</files>
  <action>
    Create a migration script that standardizes existing bot RAG tables to the `{schema}.document_chunks` convention.

    **Structure the SQL in sections with clear comments:**

    1. **Pre-flight checks** — Verify source tables exist before migrating. Use DO $$ blocks to check and print/raise notices.

    2. **WaterBot migration:**
       - `CREATE SCHEMA IF NOT EXISTS waterbot;`
       - Create `waterbot.document_chunks` with standard columns
       - INSERT INTO ... SELECT from `public.waterbot_documents`, mapping:
         - `content` → `chunk_text`
         - `metadata->>'category'` → `category`
         - `metadata->>'subcategory'` → `subcategory`
         - `metadata->>'file_path'` → `file_path`
         - `metadata->>'file_name'` → `file_name`
         - `metadata->>'char_count'` → `char_count` (cast to integer)
         - `metadata->>'section_title'` → `section_title`
         - `metadata` → `metadata` (keep full JSONB for anything extra)
         - `embedding` → `embedding`
         - `md5(content)` → `content_hash`
       - Create HNSW index on new table
       - DO NOT drop `public.waterbot_documents` — add comment: "Drop after verification: DROP TABLE public.waterbot_documents;"

    3. **BizBot migration:**
       - `CREATE SCHEMA IF NOT EXISTS bizbot;`
       - Create `bizbot.document_chunks` with standard columns PLUS BizBot-specific columns:
         - `topic VARCHAR(50)`, `industries TEXT[]`, `agencies TEXT[]`, `cities TEXT[]`, `counties TEXT[]`
         - `chunk_level VARCHAR(20)`, `verification_status VARCHAR(20)`, `confidence FLOAT`
       - INSERT INTO ... SELECT from `public.bizbot_documents`, mapping columns directly (most already match)
       - Preserve BizBot's richer metadata (arrays, verification status, confidence) in dedicated columns
       - Create HNSW index
       - DO NOT drop original table

    4. **KiddoBot — no migration needed:**
       - Add comment: "kiddobot.document_chunks already uses standard naming — no migration required"
       - Optionally add missing standard columns (content_hash, metadata JSONB) via ALTER TABLE IF NOT EXISTS

    5. **Verification queries:**
       - Row count comparison: source vs destination for each bot
       - Sample data spot-check: first 3 rows from each new table
       - Embedding dimension verification

    **What to avoid:**
    - Do NOT DROP original tables — migration is copy-first, verify, then manual drop
    - Do NOT use transactions wrapping everything (each bot migration should be independent — if BizBot fails, WaterBot shouldn't rollback)
    - Do NOT assume WaterBot data is in postgres database — discovery showed it may be in n8n database. Add a comment noting this and that the script targets the postgres database. If WaterBot data is in a different database, a separate connection is needed.
    - Do NOT create new constraints that would reject existing data (e.g., NOT NULL on columns that may have NULLs in source)

    **Important cross-database note:**
    WaterBot's `public.waterbot_documents` may be in the `n8n` database, not `postgres`. Add a clearly-commented section at the top explaining:
    - If both source and target are in the same database: run the script directly
    - If WaterBot data is in a different database: use pg_dump/pg_restore or a separate Python migration script
    - Include the alternative approach as commented-out code
  </action>
  <verify>
    ```bash
    # SQL file exists and has substance
    wc -l factory/scripts/migrate-rag-tables.sql

    # Contains all three bot sections
    grep -c "waterbot\|bizbot\|kiddobot" factory/scripts/migrate-rag-tables.sql

    # Contains safety checks (no DROP without comment)
    grep -i "DROP TABLE" factory/scripts/migrate-rag-tables.sql || echo "No DROP TABLE ✓"
    ```
  </verify>
  <done>Migration SQL has all 3 bot sections. WaterBot: JSONB metadata → explicit columns. BizBot: preserves rich columns (arrays, verification). KiddoBot: noted as already correct, optional ALTER TABLE for missing columns. Cross-database note included. No DROP statements — copy-first strategy. Verification queries at end.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>RAG table migration SQL that will restructure production bot data on the VPS</what-built>
  <how-to-verify>
    1. Read: `cat factory/scripts/migrate-rag-tables.sql`
    2. Verify WaterBot column mapping looks correct (content→chunk_text, metadata extraction)
    3. Verify BizBot preserves all rich columns (arrays, verification_status, confidence)
    4. Verify NO DROP TABLE statements exist (copy-first safety)
    5. Verify cross-database note addresses WaterBot's potential n8n database location
    6. **Do NOT execute yet** — this is review only. Execution happens manually after Phase 1 complete.
  </how-to-verify>
  <resume-signal>Type "approved" to complete Phase 1, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] TEMPLATE.md exists with YAML frontmatter and section structure
- [ ] decision-tree-schema.json is valid JSON Schema
- [ ] migrate-rag-tables.sql covers WaterBot + BizBot migration
- [ ] Migration is copy-first (no DROP statements)
- [ ] Human reviewed migration SQL
</verification>

<success_criteria>

- Knowledge template establishes authoring standard for all future bots
- Decision tree schema formalizes the structure used by WaterBot/KiddoBot
- Migration SQL can standardize existing bots to `{schema}.document_chunks`
- Phase 1 complete — factory directory, RAG pipeline, templates, migration all in place
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md` with:
- Note this is the final plan of Phase 1
- "Phase 1 complete" in Next Step section
</output>
