---
phase: 01-foundation
plan: 02
type: execute
---

<objective>
Create the three RAG pipeline scripts: chunk-knowledge.js, embed-chunks.py, and validate-knowledge.py.

Purpose: Standardize the RAG ingest pipeline so every bot project uses the same proven tooling instead of ad-hoc per-bot scripts.
Output: Three working scripts in factory/scripts/ that can process any bot's knowledge directory.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-01-SUMMARY.md

**Key source files to generalize from:**
@/Users/slate/Documents/GitHub/CA-AIDev/waterbot/scripts/chunk-knowledge.js
@/Users/slate/Documents/GitHub/CA-AIDev/waterbot/scripts/embed-chunks.py

**Tech available:** Node.js (standard libs only for chunker), Python 3 (openai, psycopg2 for embedder)
**Embedding model:** OpenAI text-embedding-3-small, 1536 dimensions
**Standard table:** `{schema}.document_chunks`

**Constraining decisions:**
- WaterBot chunker uses split-on-H2, keep-H3-with-parent, prefix-H1, max 2000 chars — preserve this proven logic
- WaterBot embedder has HARDCODED credentials (IP: 100.111.63.3, password in plaintext) — factory version MUST use env vars exclusively
- BizBot has richer schema (content_hash, verification_status, confidence) — adopt best of both in standard table
- KiddoBot already uses `kiddobot.document_chunks` naming — this is the target standard

**Current WaterBot hardcoded values to parameterize:**
- `KNOWLEDGE_DIR` path → `--knowledge-dir` flag
- `OUTPUT_FILE` path → `--output` flag
- Collection name "waterbot" → `--collection` flag
- DB host, password, table name → env vars + `--schema`/`--table` flags
- "WaterBot" in console output → generic or parameterized

**Standard column schema for document_chunks:**
```sql
id SERIAL PRIMARY KEY,
document_id VARCHAR(255),
chunk_text TEXT NOT NULL,
chunk_index INTEGER DEFAULT 0,
file_name VARCHAR(255),
file_path VARCHAR(500),
category VARCHAR(100),
subcategory VARCHAR(100),
section_title VARCHAR(255),
char_count INTEGER,
content_hash VARCHAR(64),  -- md5(chunk_text)
embedding vector(1536),
metadata JSONB DEFAULT '{}',
created_at TIMESTAMP DEFAULT NOW(),
updated_at TIMESTAMP DEFAULT NOW()
```
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chunk-knowledge.js</name>
  <files>factory/scripts/chunk-knowledge.js</files>
  <action>
    Generalize WaterBot's `chunk-knowledge.js` into a reusable factory script.

    **CLI interface:**
    ```
    node chunk-knowledge.js --knowledge-dir ../knowledge --output chunks.json --collection waterbot
    ```
    All three flags required. No defaults that assume a specific bot.

    **Core logic (preserve from WaterBot — it works):**
    - Walk `--knowledge-dir` recursively for .md files
    - Extract H1 as document title
    - Split on `## ` (H2 headers) for chunk boundaries
    - Keep `### ` (H3) subsections with their parent H2
    - Prefix each chunk with document title for retrieval context
    - Enforce MAX_CHUNK_SIZE=2000 chars (split oversized on paragraph `\n\n` boundaries)
    - Skip chunks under MIN_CHUNK_SIZE=100 chars

    **New: YAML frontmatter parsing:**
    - If document starts with `---`, parse YAML frontmatter (between first two `---` markers)
    - Extract: title, domain, category, source_authority, source_urls, last_verified
    - Inject frontmatter fields into chunk metadata (every chunk from that doc gets the doc's frontmatter)
    - If no frontmatter, fall back to path-based metadata extraction (current WaterBot behavior)

    **Metadata per chunk:**
    ```json
    {
      "document_id": "[collection]-[filename-slug]",
      "chunk_text": "[prefixed content]",
      "chunk_index": 0,
      "file_name": "wdr-overview.md",
      "file_path": "03-permits/wdr/wdr-overview.md",
      "category": "permits",
      "subcategory": "wdr",
      "section_title": "General Orders",
      "char_count": 1847,
      "collection": "waterbot",
      "frontmatter": { ... }
    }
    ```

    **Output:** JSON array to `--output` path. Print summary: total docs processed, total chunks, avg chunk size, min/max chunk size.

    **What to avoid:**
    - Do NOT use any npm packages — standard Node.js libs only (fs, path, process). This keeps the script zero-dependency.
    - Do NOT hardcode any bot name, path, or collection — everything via CLI flags.
    - Do NOT silently skip files on error — log warnings with file path.

    Use `process.argv` parsing (no commander/yargs — zero deps). Simple `--flag value` parsing is sufficient.
  </action>
  <verify>
    ```bash
    # Test against WaterBot's actual knowledge directory
    node factory/scripts/chunk-knowledge.js \
      --knowledge-dir /Users/slate/Documents/GitHub/CA-AIDev/waterbot/knowledge \
      --output /tmp/test-chunks.json \
      --collection waterbot

    # Verify output
    python3 -c "import json; d=json.load(open('/tmp/test-chunks.json')); print(f'Chunks: {len(d)}'); print(f'First chunk keys: {list(d[0].keys())}')"

    # Cleanup
    rm /tmp/test-chunks.json
    ```
  </verify>
  <done>Script produces valid chunks.json from WaterBot's knowledge dir. Each chunk has all required metadata fields. Collection name matches --collection flag. No hardcoded bot references.</done>
</task>

<task type="auto">
  <name>Task 2: Create embed-chunks.py</name>
  <files>factory/scripts/embed-chunks.py</files>
  <action>
    Generalize WaterBot's `embed-chunks.py` into a reusable factory script.

    **CLI interface:**
    ```
    python3 embed-chunks.py --chunks chunks.json --schema waterbot --table document_chunks
    ```
    `--table` defaults to `document_chunks` (the standard). `--schema` required, `--chunks` required.

    **Environment variables (ALL credentials from env — ZERO hardcoded):**
    - `DB_HOST` (required)
    - `DB_PORT` (default: 5432)
    - `DB_NAME` (default: postgres)
    - `DB_USER` (default: postgres)
    - `DB_PASSWORD` (required)
    - `OPENAI_API_KEY` (required)

    Script MUST check all required env vars exist at startup and exit with clear error message if missing. No fallback to hardcoded values.

    **Core logic:**
    1. Read chunks JSON from `--chunks`
    2. Connect to PostgreSQL via env vars
    3. Ensure schema exists: `CREATE SCHEMA IF NOT EXISTS {schema}`
    4. Ensure table exists with standard columns (CREATE TABLE IF NOT EXISTS):
       ```sql
       {schema}.{table} (
         id SERIAL PRIMARY KEY,
         document_id VARCHAR(255),
         chunk_text TEXT NOT NULL,
         chunk_index INTEGER DEFAULT 0,
         file_name VARCHAR(255),
         file_path VARCHAR(500),
         category VARCHAR(100),
         subcategory VARCHAR(100),
         section_title VARCHAR(255),
         char_count INTEGER,
         content_hash VARCHAR(64),
         embedding vector(1536),
         metadata JSONB DEFAULT '{}',
         created_at TIMESTAMP DEFAULT NOW(),
         updated_at TIMESTAMP DEFAULT NOW()
       )
       ```
    5. Create HNSW vector index if not exists: `CREATE INDEX IF NOT EXISTS idx_{schema}_{table}_embedding ON {schema}.{table} USING hnsw (embedding vector_cosine_ops) WITH (m=16, ef_construction=64)`
    6. Process chunks in batches of 100:
       - Call OpenAI embeddings API (text-embedding-3-small, 1536 dims)
       - Compute content_hash as md5(chunk_text)
       - INSERT with ON CONFLICT (skip duplicates by content_hash using unique constraint)
    7. Print summary: total embedded, duplicates skipped, total rows in table

    **What to avoid:**
    - NEVER hardcode IP addresses, passwords, or credentials (the WaterBot script has `"password": "2LofmsGNMYUfgF6bGPoFmdcU6M4"` — this is a security issue we're fixing)
    - Do NOT TRUNCATE table by default — use upsert/skip-duplicates instead. Add `--fresh` flag to opt-in to truncate-first behavior.
    - Do NOT auto-install packages via pip in the script (WaterBot does this). Instead, print clear error if openai/psycopg2 missing with install command.

    **Dependencies:** openai, psycopg2-binary (user installs, script checks)
  </action>
  <verify>
    ```bash
    # Verify script parses args and checks env vars (dry run — will fail on missing env but that's the test)
    python3 factory/scripts/embed-chunks.py --help 2>&1 || true

    # Verify it exits cleanly with missing env vars
    python3 factory/scripts/embed-chunks.py --chunks /tmp/test.json --schema test 2>&1 | head -5

    # Check no hardcoded credentials
    grep -n "password\|2LofmsGN\|100\.111" factory/scripts/embed-chunks.py || echo "No hardcoded credentials found ✓"
    ```
  </verify>
  <done>Script accepts --chunks, --schema, --table flags. All DB creds from env vars. Zero hardcoded credentials (grep confirms). Creates schema + table + HNSW index automatically. Batch embedding with duplicate detection via content_hash. --fresh flag for truncate behavior.</done>
</task>

<task type="auto">
  <name>Task 3: Create validate-knowledge.py</name>
  <files>factory/scripts/validate-knowledge.py</files>
  <action>
    New script implementing the RAG Quality Gate checks.

    **CLI interface:**
    ```
    python3 validate-knowledge.py --schema waterbot --table document_chunks
    ```
    Same env var pattern as embed-chunks.py for DB connection.

    **Checks (each reports PASS/FAIL):**

    1. **Dedup check:** `SELECT COUNT(*) - COUNT(DISTINCT md5(chunk_text)) FROM {schema}.{table}` — must equal 0
    2. **NULL embedding check:** `SELECT COUNT(*) FROM {schema}.{table} WHERE embedding IS NULL` — must equal 0
    3. **Embedding dimension check:** Verify all embeddings are 1536 dimensions
    4. **Chunk size distribution:** Report min, max, avg, median char_count. Flag if any chunks >3000 or <50 chars
    5. **URL validation:** Extract URLs from chunk_text using regex, test each with HTTP HEAD request, report non-2xx responses. Use `urllib.request` (no requests package needed). Timeout 10s per URL. Deduplicate URLs before checking.
    6. **Content hash integrity:** Verify stored content_hash matches md5(chunk_text) for all rows

    **Output format:**
    ```
    === RAG Knowledge Validation ===
    Schema: waterbot | Table: document_chunks
    Total chunks: 1253

    [PASS] Dedup check: 0 duplicates
    [PASS] NULL embeddings: 0 found
    [PASS] Embedding dimensions: all 1536
    [INFO] Chunk sizes: min=127, max=1984, avg=1203, median=1156
    [WARN] URL check: 2/47 URLs returned non-2xx (see details below)
    [PASS] Content hash integrity: all match

    Overall: 5/6 PASS, 0 FAIL, 1 WARN
    ```

    Exit code 0 if all PASS/WARN, exit code 1 if any FAIL.

    **What to avoid:**
    - Do NOT require `requests` package — use `urllib.request` from standard library
    - Do NOT fail the entire validation on URL timeout — report as WARN, not FAIL
    - Do NOT run URL checks by default if there are >100 unique URLs — add `--skip-urls` flag and warn

    **Dependencies:** psycopg2-binary only (same as embed script)
  </action>
  <verify>
    ```bash
    # Verify script parses and checks env
    python3 factory/scripts/validate-knowledge.py --help 2>&1 || true

    # Check it's self-contained (no requests dependency)
    grep -n "import requests" factory/scripts/validate-knowledge.py || echo "No requests dependency ✓"
    ```
  </verify>
  <done>Script runs all 6 validation checks against any schema.table. Clear PASS/FAIL/WARN output. Exit code 1 on any FAIL. URL validation uses stdlib only. --skip-urls flag available.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `chunk-knowledge.js` processes WaterBot's knowledge dir into valid JSON
- [ ] `embed-chunks.py` has zero hardcoded credentials (grep confirms)
- [ ] `embed-chunks.py` creates schema + table + index automatically
- [ ] `validate-knowledge.py` runs all 6 checks with clear output
- [ ] All three scripts have `--help` output documenting their flags
- [ ] No hardcoded bot names, paths, or credentials in any script
</verification>

<success_criteria>

- Three scripts in factory/scripts/ that work with any bot's knowledge
- chunk-knowledge.js produces valid JSON from WaterBot's knowledge dir
- embed-chunks.py uses env vars exclusively for credentials
- validate-knowledge.py implements RAG Quality Gate (dedup + URLs + stats)
- All committed to git
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
