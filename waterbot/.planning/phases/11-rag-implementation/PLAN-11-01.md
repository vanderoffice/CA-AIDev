# PLAN-11-01: Document Processing & Chunking

## Objective
Process 130 knowledge markdown files into chunks suitable for embedding and vector storage.

## Prerequisites
- 130 knowledge files in `knowledge/` across phases 03-10
- `waterbot.document_chunks` table exists in Supabase (verified Phase 2)

## Chunking Strategy

### Markdown-Aware Chunking
Unlike generic text splitting, our knowledge files have semantic structure we should preserve:

1. **Header-based splits**: Each H2/H3 section becomes its own chunk
2. **Maximum chunk size**: ~1500 characters (leaves room for section context)
3. **Overlap**: Include parent header in each chunk for context
4. **Metadata extraction**: 
   - `category`: from folder path (e.g., `03-permits` → `permits`)
   - `subcategory`: from subfolder (e.g., `npdes/` → `npdes`)
   - `file_name`: original markdown filename
   - `file_path`: relative path for reference

### Example
```
knowledge/03-permits/npdes/npdes-overview.md
  Section: "## What is NPDES?"
  → chunk_text: "## What is NPDES?\n\nThe National Pollutant Discharge..."
  → category: "permits"
  → subcategory: "npdes"
  → document_id: "03-permits/npdes/npdes-overview"
  → chunk_index: 0
```

## Tasks

### 1. Inventory Knowledge Files
```bash
# Count files per category
find knowledge/ -name "*.md" | wc -l
# Expected: 130 files
```

### 2. Create Chunking Script
Create Node.js script at `scripts/chunk-knowledge.js`:
- Read all markdown files from `knowledge/`
- Parse markdown structure
- Split on H2 headers (keep H3 nested within H2 chunks)
- Output JSON array of chunk objects
- Save to `scripts/chunks.json` for inspection before loading

### 3. Validate Output
- Review sample chunks for coherence
- Verify metadata extraction
- Check chunk sizes (target: 500-1500 chars)
- Ensure no empty chunks

## Output
- `scripts/chunk-knowledge.js` - Chunking script
- `scripts/chunks.json` - Processed chunks ready for embedding
- Chunk statistics: total count, avg size, distribution by category

## Success Criteria
- [ ] All 130 files processed without errors
- [ ] Chunks maintain semantic coherence (not mid-sentence splits)
- [ ] Metadata accurately reflects source location
- [ ] Output JSON validated and ready for PLAN-11-02

## Notes
- This plan is compute-only (no API calls)
- Embedding generation deferred to PLAN-11-02 to manage costs
- Can re-run chunking if strategy needs adjustment
