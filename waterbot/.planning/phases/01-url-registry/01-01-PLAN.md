---
phase: 01-url-registry
plan: 01
type: execute
---

<objective>
Create the URL registry JSON schema, build a complete topic inventory from all 33 WaterBot batch files, extract existing URLs as a baseline, and map the CA gov domain structure for efficient research in subsequent plans.

Purpose: Establish the registry infrastructure and research methodology before bulk URL population — get the schema right early so Plans 02-03 can focus purely on research.
Output: `rag-content/waterbot/url_registry.json` with schema, 179-topic inventory, baseline URLs extracted from existing content, and a site-mapping section documenting CA gov URL patterns.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/phase-prompt.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Existing content files (33 batch JSONs):**
@rag-content/waterbot/

**Existing URL validation infrastructure:**
@scripts/test_all_urls.py
@scripts/test_urls_from_files.py
@url_validation_results.json

**Key facts:**
- 179 documents across 33 JSON files (12 multi-doc batches + 21 single-doc files)
- 21 files already contain URLs (~194 URLs total), 12 batch files have ZERO URLs
- 11 categories: Water Quality (40), Permits (40), Programs (84), Compliance (28), Funding (22), Public Resources (21), Consumer FAQ (12), Regional Boards (10), Small Systems (10), Community Resources (8), Water Supply (5)
- Each document has metadata: category, subcategory, topic
- PROJECT.md constraint: prefer stable parent pages over deep links (CA gov restructures frequently)
- Existing URL validation passed: 193/194 waterbot URLs OK, 0 broken
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create URL registry schema and extract baseline data</name>
  <files>rag-content/waterbot/url_registry.json</files>
  <action>
  Create `rag-content/waterbot/url_registry.json` with this structure:

  Top-level object with:
  - `schema_version`: "1.0"
  - `last_updated`: ISO date string
  - `stats`: object with total_topics, total_urls, validated_count, broken_count
  - `entries`: array of topic entry objects

  Each topic entry:
  - `topic`: string (exact topic from batch file metadata)
  - `category`: string (from metadata)
  - `subcategory`: string (from metadata)
  - `source_file`: string (which batch JSON file this topic comes from)
  - `urls`: array of URL objects

  Each URL object:
  - `label`: string (human-readable description, e.g. "SWRCB PFAS Program Page")
  - `url`: string (full URL)
  - `type`: one of "info", "portal", "form", "database", "contact", "regulation", "tool"
  - `agency`: string (e.g. "SWRCB", "DDW", "EPA", "DWR", "OEHHA", "Regional Board")
  - `stable`: boolean (true for parent/landing pages, false for deep links that may change)

  Build the complete 179-topic inventory by parsing ALL 33 JSON files in rag-content/waterbot/. For each document, create an entry with the metadata fields populated and urls array initially empty.

  Then extract existing URLs from files that already contain them. Parse each document's content string for http/https URLs using regex. For each found URL, create a URL object in the corresponding topic entry. Set type and agency based on URL domain:
  - waterboards.ca.gov → agency "SWRCB" or "Regional Board" depending on path
  - waterboards.ca.gov/drinking_water → agency "DDW"
  - epa.gov → agency "EPA"
  - water.ca.gov → agency "DWR"
  - oehha.ca.gov → agency "OEHHA"

  Do NOT use Python to build this — use shell/jq or manual construction. The output must be valid JSON.
  </action>
  <verify>
  - `python3 -c "import json; d=json.load(open('rag-content/waterbot/url_registry.json')); print(f'{len(d[\"entries\"])} topics, {sum(len(e[\"urls\"]) for e in d[\"entries\"])} URLs')"` shows 179 topics
  - All 11 categories present in entries
  - Baseline URLs extracted from existing files
  </verify>
  <done>Valid JSON registry with 179 topic entries, baseline URLs populated from existing content, stats section accurate</done>
</task>

<task type="auto">
  <name>Task 2: Map CA gov domain structure for efficient research</name>
  <files>rag-content/waterbot/url_registry.json</files>
  <action>
  Research the primary CA gov domains that WaterBot topics map to. Navigate each domain's site structure to understand URL patterns and identify stable landing pages.

  Primary domains to map:
  1. **waterboards.ca.gov** — SWRCB main site. Key sections: /drinking_water/, /water_issues/programs/, /resources/fees/, /safer/, each regional board path
  2. **waterboards.ca.gov/drinking_water/** — DDW. Key sections: certlic/, programs/, chemicals/ (contaminant pages)
  3. **water.ca.gov** — DWR. Key sections: programs/, water-basics/
  4. **epa.gov** — EPA. Key sections: /ground-water-and-drinking-water/, /sdwa/, /dwstandardsregulations
  5. **oehha.ca.gov** — OEHHA. Key sections: water/ (public health goals)
  6. **Each regional board** — waterboards.ca.gov/[region]/ (northcoast, sanfranciscobay, centralcoast, losangeles, centralvalley, lahontan, coloradoriver, santaana, sandiego)

  For each domain section, document:
  - The base URL pattern
  - What WaterBot categories/subcategories it serves
  - Which specific topics map to which subsections
  - Whether URLs use stable paths or dynamic/ID-based paths

  Add this mapping as a `site_map` top-level object in url_registry.json with structure:
  ```
  "site_map": {
    "waterboards.ca.gov": {
      "base": "https://www.waterboards.ca.gov",
      "sections": {
        "drinking_water": { "path": "/drinking_water/", "serves": ["Water Quality", "Compliance"], "stable": true },
        ...
      }
    },
    ...
  }
  ```

  Use WebSearch and WebFetch to navigate these domains. Focus on identifying stable URL patterns, not exhaustively crawling every page. Prefer HTTPS URLs. Note any domains that redirect (e.g. some SWRCB pages redirect to new structures).

  This site map will be the research guide for Plans 01-02 and 01-03 — it tells the executor WHERE to look for each topic category's URLs instead of searching blindly.
  </action>
  <verify>
  - site_map object exists in url_registry.json with entries for all 6 primary domains
  - Each domain section documents which WaterBot categories it serves
  - Regional board URL patterns documented for all 9 regions
  - JSON remains valid after additions
  </verify>
  <done>Site map documents URL patterns for all primary CA gov domains, with clear mapping to WaterBot topic categories. Research guide ready for Plans 02-03.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `url_registry.json` is valid JSON (python3 -c "import json; json.load(open(...))")
- [ ] 179 topic entries present with correct category/subcategory/topic metadata
- [ ] Baseline URLs extracted from existing content files
- [ ] Site map covers waterboards.ca.gov, water.ca.gov, epa.gov, oehha.ca.gov, all 9 regional boards
- [ ] Stats section reflects actual counts
</verification>

<success_criteria>

- Registry JSON created with well-defined schema
- All 179 topics inventoried from batch files
- Existing URLs extracted as baseline (~194 URLs)
- CA gov domain structure mapped for efficient research
- Site map links domain sections to WaterBot categories
</success_criteria>

<output>
After completion, create `.planning/phases/01-url-registry/01-01-SUMMARY.md`
</output>
