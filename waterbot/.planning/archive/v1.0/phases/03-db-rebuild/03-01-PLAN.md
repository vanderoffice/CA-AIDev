---
phase: 03-db-rebuild
plan: 01
type: execute
---

<objective>
Audit the current WaterBot database and inventory the new overhauled content to determine the optimal rebuild strategy — clean slate (TRUNCATE + re-ingest) vs preserve+enhance (keep old rows, add/replace selectively).

Purpose: The rebuild strategy determines whether we wipe the database and start fresh or surgically update it. Wrong choice wastes time (preserve+enhance when everything changed) or risks data loss (clean slate when old content has unique value). Data-driven decision before touching production.
Output: Coverage analysis report, strategy decision documented in summary.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Auto-selected based on dependency graph:
@.planning/phases/02-content-overhaul/02-04-SUMMARY.md

# Key files:
@scripts/ingest_waterbot_content.py
@rag-content/waterbot/

**Tech stack available:** Python 3, OpenAI embeddings (text-embedding-3-small, 1536 dims), PostgreSQL 15.8.1 + pgvector 0.8.0, SSH pipeline (ssh vps "docker exec -i supabase-db psql -U postgres -d postgres")
**Established patterns:** SSH + docker exec + psql for all DB operations, dollar-quoted SQL with CONTENT_END_12345 tag, IVFFlat REINDEX after bulk inserts
**Constraining decisions:**
- Phase 01: 179 topics with 577 verified URLs in url_registry.json
- Phase 02: All 179 topics rewritten across 33 JSON files with inline URLs and Take Action sections (1,171 total links)
- Phase 02-04: Gap filler docs expanded 3.9x (avg 2,020 chars) — more content volume than baseline
- Out of scope: Embedding model change, schema migration (keep content/metadata/embedding columns)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Audit current DB state and inventory new content</name>
  <files>scripts/ (read only), rag-content/waterbot/*.json (read only)</files>
  <action>
  **Remote DB audit (SSH to VPS):**
  - Total row count: `SELECT COUNT(*) FROM public.waterbot_documents;`
  - Old-schema rows (markdown-chunked): `SELECT COUNT(*) FROM public.waterbot_documents WHERE metadata ? 'document_id';`
  - New-schema rows (batch JSON): `SELECT COUNT(*) FROM public.waterbot_documents WHERE metadata ? 'topic';`
  - Metadata category distribution: `SELECT metadata->>'category' AS cat, COUNT(*) FROM public.waterbot_documents GROUP BY 1 ORDER BY 2 DESC;`
  - Sample 2 old-schema rows to check for URL presence: `SELECT LEFT(content, 500) FROM public.waterbot_documents WHERE metadata ? 'document_id' LIMIT 2;`
  - Sample 2 new-schema rows to compare: `SELECT LEFT(content, 500) FROM public.waterbot_documents WHERE metadata ? 'topic' LIMIT 2;`

  **Local content inventory:**
  - Parse every JSON file in `rag-content/waterbot/` EXCEPT `url_registry.json` (different structure — not content)
  - Count total documents: sum of `len(data['documents'])` across all files
  - Report: total docs, file count, avg content length
  - Verify the ingestion script's glob pattern — does `ingest_waterbot_content.py` filter out `url_registry.json` or would it choke on the different structure?

  **Coverage comparison:**
  - Do the 179 batch topics fully supersede the old markdown-chunked content?
  - Sample old-schema `section_title` / `document_id` values to check for unique content not in batch files
  - Check old content for URLs (expect very few — this was the whole motivation for the overhaul)

  Present findings as a clear summary table.
  </action>
  <verify>All SSH queries return results (no connection errors). Local file count matches ~179 expected topics. Coverage analysis has a clear conclusion.</verify>
  <done>Audit report showing: (1) current DB rows by schema type, (2) new content doc count and file count, (3) coverage overlap assessment, (4) URL presence comparison between old and new content</done>
</task>

<task type="checkpoint:decision" gate="blocking">
  <decision>Select WaterBot database rebuild strategy</decision>
  <context>Phase 2 rewrote all 179 topics with inline URLs and Take Action sections. The current DB has ~1,286 rows — mostly old markdown-chunked content created before the URL overhaul. The rebuild strategy determines how we transition.</context>
  <options>
    <option id="clean-slate">
      <name>Clean Slate (TRUNCATE + re-ingest)</name>
      <pros>Simple — one script run. No dedup needed. Guarantees 100% URL-rich content. Consistent metadata schema (topic/category/subcategory). Smaller, cleaner DB. Only embeds new content (lower OpenAI cost).</pros>
      <cons>Brief window where DB is empty during ingest (~5-10 min). Loses old content not covered by batch files (if any). Loses created_at timestamps on old rows.</cons>
    </option>
    <option id="preserve-enhance">
      <name>Preserve + Enhance (keep old, add new alongside)</name>
      <pros>No empty-DB window. Preserves any unique old content not in new files.</pros>
      <cons>Complex dedup — old and new content overlap semantically. Mixed metadata schemas hurt query consistency. DB bloat (~1,465 rows with redundant coverage). LLM may retrieve old URL-less version instead of new URL-rich version. Higher embedding cost (old + new). More work for uncertain benefit.</cons>
    </option>
  </options>
  <resume-signal>Select: clean-slate or preserve-enhance</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] DB audit queries executed successfully via SSH
- [ ] New content inventory counted all JSON files
- [ ] Coverage comparison completed with clear conclusion
- [ ] User selected rebuild strategy
- [ ] Decision documented in summary
</verification>

<success_criteria>

- DB audit data collected (row counts by schema type, category distribution, content samples)
- New content inventory complete (document count, file count, avg length)
- Coverage comparison shows whether old content has unique value vs new batch content
- Rebuild strategy decided by user
- Decision rationale documented
</success_criteria>

<output>
After completion, create `.planning/phases/03-db-rebuild/03-01-SUMMARY.md`
</output>
