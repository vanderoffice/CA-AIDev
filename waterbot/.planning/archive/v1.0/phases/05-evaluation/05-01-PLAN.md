---
phase: 05-evaluation
plan: 01
type: execute
---

<objective>
Run full adversarial evaluation and URL validation against the rebuilt WaterBot database.

Purpose: Generate the post-overhaul evaluation data — retrieval similarity scores, URL health, and deferred issue investigation — that Plan 05-02 will analyze against the baseline.
Output: New adversarial evaluation JSON, URL validation JSON, deferred issue analysis, URL coverage counts.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase summaries (dependency graph):
@.planning/phases/03-db-rebuild/03-02-SUMMARY.md
@.planning/phases/04-system-prompt/04-01-SUMMARY.md

# Key scripts:
@scripts/run_adversarial_evaluation.py
@scripts/test_all_urls.py

# Baseline for comparison (Plan 05-02 will use):
@.planning/adversarial_evaluation_20260210_152315.json

# Test set:
@.planning/adversarial_test_set.json

**Tech stack available:** Python 3 (.venv), OpenAI API (text-embedding-3-small), SSH to VPS, PostgreSQL/pgvector
**Established patterns:** SSH piped SQL queries, .venv/bin/python3 for scripts needing openai module, parallel HTTP validation

**Constraining decisions:**
- Phase 03-02: Clean-slate rebuild — DB now has 179 rows (was 1,286). Baseline comparison is old DB vs new DB.
- Phase 04-01: Credential bug fixed, token limit 1000→2000, temp 0.3→0.2. RAG retrieval is now working.
- Phase 01-04: 403 responses from CA gov sites = OK (bot protection, URLs work in browsers)

**Deferred issues to investigate:**
- "Conservation as a Way of Life" query doesn't retrieve doc 50 (below 0.50 similarity) — check against 0.40 STRONG threshold
- "File complaint" query retrieves no docs — content gap
- `file_name` metadata always null in waterbot_documents — cosmetic, document only
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run full adversarial evaluation</name>
  <files>scripts/run_adversarial_evaluation.py, .planning/adversarial_evaluation_*.json</files>
  <action>Run the adversarial evaluation script using `.venv/bin/python3 scripts/run_adversarial_evaluation.py` from the project root. This runs all 3 bots (bizbot, kiddobot, waterbot) against the 35-query adversarial test set. Let it complete — it takes ~2-3 minutes (35 queries × 3 bots × OpenAI embedding calls + SSH queries). Capture the output file path. Print waterbot summary (strong/acceptable/weak counts) and flag any queries that scored WEAK or NO_RESULTS. Note: bizbot and kiddobot results serve as a control — they should be unchanged since we only modified waterbot_documents.</action>
  <verify>New adversarial_evaluation_*.json file exists in .planning/ with run_date matching today. WaterBot section has 35 results. No NO_RESULTS scores (would indicate DB connectivity issue).</verify>
  <done>Evaluation JSON saved with all 35 waterbot queries scored. Summary printed showing strong/acceptable/weak breakdown.</done>
</task>

<task type="auto">
  <name>Task 2: Run URL validation on live DB content</name>
  <files>scripts/test_all_urls.py, url_validation_results.json</files>
  <action>Run `.venv/bin/python3 scripts/test_all_urls.py` from project root. This extracts all URLs from waterbot_documents via SSH regex query, then tests each URL with parallel HTTP requests (20 workers). The script tests all 3 bots. Key context: CA gov sites often return 403 (bot protection) — these are NOT broken, just blocked for automated requests. True failures are DNS errors and 5xx responses. After the script completes, also run a SQL query to count URL coverage per document: `ssh vps "docker exec -i supabase-db psql -U postgres -d postgres -t -A -c \"SELECT COUNT(*) as total_docs, COUNT(CASE WHEN content::text ~ 'https?://' THEN 1 END) as docs_with_urls FROM public.waterbot_documents;\""` — this confirms every document has at least one URL.</action>
  <verify>url_validation_results.json exists with waterbot section. SQL query returns docs_with_urls = total_docs (179/179). No DNS failures in waterbot URLs.</verify>
  <done>URL validation complete. All waterbot URLs tested. URL coverage = 100% (all 179 docs contain URLs). Any true failures (DNS, 5xx) documented.</done>
</task>

<task type="auto">
  <name>Task 3: Investigate deferred issues</name>
  <files>.planning/phases/05-evaluation/deferred_issue_analysis.md</files>
  <action>Test the three deferred issues from STATE.md:

1. **"Conservation as a Way of Life"** — Generate embedding for this query and run similarity search against waterbot_documents. Check if any result scores ≥ 0.40 (STRONG) or ≥ 0.30 (ACCEPTABLE). The prior issue noted "below 0.50" which is above our STRONG threshold — may actually be fine.

2. **"File complaint"** — Generate embedding for "How do I file a complaint about my water?" and run similarity search. Check if any docs are retrieved. Cross-reference with wat-013 ("Who do I complain to if my water looks or smells bad?") which scored 0.542 STRONG in baseline. Determine if this is a phrasing sensitivity issue or a genuine content gap.

3. **`file_name` metadata null** — Run SQL: `SELECT DISTINCT metadata->>'file_name' FROM public.waterbot_documents LIMIT 10;` to confirm. This is cosmetic (only affects source attribution display). Document but don't fix.

Write results to `.planning/phases/05-evaluation/deferred_issue_analysis.md` with: query tested, similarity scores, assessment (resolved/still-open/cosmetic), and recommendation.

Use `.venv/bin/python3` for any scripts that need the openai module. For quick one-off similarity queries, use a small inline Python script that: imports openai, generates embedding, constructs SQL, pipes to SSH.</action>
  <verify>deferred_issue_analysis.md exists with all 3 issues investigated. Each has similarity scores or SQL results. Each has a clear assessment.</verify>
  <done>All 3 deferred issues investigated with evidence. Conservation query assessed against STRONG/ACCEPTABLE thresholds. File complaint query tested with similarity scores. file_name null confirmed and documented.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] New adversarial_evaluation_*.json exists with today's date
- [ ] WaterBot has 35 scored results (no NO_RESULTS)
- [ ] url_validation_results.json exists with waterbot data
- [ ] URL coverage SQL confirms 179/179 docs have URLs
- [ ] deferred_issue_analysis.md covers all 3 issues
- [ ] BizBot and KiddoBot results present as control (unchanged from prior runs)
</verification>

<success_criteria>

- Full adversarial evaluation completed and saved
- URL validation completed and saved
- All deferred issues investigated with evidence
- Raw data ready for regression analysis in Plan 05-02
</success_criteria>

<output>
After completion, create `.planning/phases/05-evaluation/05-01-SUMMARY.md`
</output>
