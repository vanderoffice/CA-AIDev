---
phase: 08-rag-pipeline-improvements
plan: 01
type: execute
---

<objective>
Add timestamp columns for chunk-level staleness tracking and enrich JSONB metadata with topic classification on all 387 chunks.

Purpose: Enable /bot-refresh staleness detection at the chunk level (not just file level) and make RAG chunks filterable by topic/industry for future retrieval improvements.
Output: bizbot_documents table with created_at/updated_at columns and enriched metadata JSONB containing topic + industry_category keys.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase summaries (dependency graph):
@.planning/phases/07-license-data-expansion/07-01-SUMMARY.md
@.planning/phases/07-license-data-expansion/07-02-SUMMARY.md

**Production DB state (verified 2026-02-16):**
- Table: `public.bizbot_documents` (NOT bizbot_chunks — schema file was aspirational)
- Columns: id (bigint), content (text), metadata (jsonb), embedding (vector)
- 387 rows, all with metadata keys: source_file, section_heading, chunk_index, content_hash
- NO timestamp columns exist yet
- bot_registry.py references `public.bizbot_documents` — this is correct

**Topic distribution from source_file paths:**
- 188 chunks (49%) in numbered directories (01_Entity through 07_Special) — trivially mappable
- 199 chunks (51%) are uncategorized (URL guides, interviews, research docs) — need manual mapping rules

**Source file → topic mapping:**
- `01_Entity_Formation/*` → entity
- `02_State_Registration/*` → state
- `03_Local_Licensing/*` → local
- `04_Industry_Requirements/*` → industry (subcategory from subdirectory name)
- `05_Environmental_Compliance/*` → environmental
- `06_Renewal_Compliance/*` → renewal
- `07_Special_Situations/*` → special
- `BizInterviews_*`, `ca_business_licensing_*` → entity
- `CA_DCA_*`, `CA_DIR_*`, `CA_FTB_*`, `california-edd-*` → state
- `Initial Assessment/*` → reference
- `research-protocol/*`, `url_fixes_*`, `README.md` → reference
- `CA_Business_Licensing_URLs_*` → reference

**Constraining decisions:**
- TEXT types in DB functions (not VARCHAR) — production schema uses text
- Production-first: all SQL runs via `ssh vps "docker exec supabase-db psql ..."`
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add timestamp columns with auto-update trigger</name>
  <files>Production DB via SSH (bizbot_documents table)</files>
  <action>
Run SQL via SSH to ALTER TABLE bizbot_documents:

1. Add `created_at` TIMESTAMPTZ DEFAULT NOW() — backfills existing rows with current time
2. Add `updated_at` TIMESTAMPTZ DEFAULT NOW()
3. Create trigger function `update_bizbot_updated_at()` that sets updated_at = NOW() on UPDATE
4. Create trigger `bizbot_documents_updated_at` BEFORE UPDATE on bizbot_documents

Use TIMESTAMPTZ (not TIMESTAMP) for timezone awareness. The DEFAULT NOW() on ALTER TABLE will backfill all 387 existing rows automatically.

Do NOT create indexes on timestamp columns — they're used for staleness queries which are infrequent batch operations, not real-time lookups. Avoid unnecessary index bloat.

Save the migration SQL to `BizBot_v4/database/05_add_timestamps.sql` in the dev repo for documentation.
  </action>
  <verify>
ssh vps 'docker exec supabase-db psql -U postgres -d postgres -c "SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '"'"'bizbot_documents'"'"' ORDER BY ordinal_position;"'

Confirm: 6 columns (id, content, metadata, embedding, created_at, updated_at)

ssh vps 'docker exec supabase-db psql -U postgres -d postgres -c "SELECT COUNT(*) as total, COUNT(created_at) as has_created, COUNT(updated_at) as has_updated FROM bizbot_documents;"'

Confirm: all 387 rows have both timestamps populated
  </verify>
  <done>bizbot_documents has created_at and updated_at columns, all 387 rows backfilled, auto-update trigger active</done>
</task>

<task type="auto">
  <name>Task 2: Enrich JSONB metadata with topic and industry_category</name>
  <files>Production DB via SSH (bizbot_documents.metadata column)</files>
  <action>
Run SQL UPDATE statements via SSH to add `topic` and `industry_category` keys to the metadata JSONB column:

1. Numbered directories — use CASE on source_file LIKE patterns:
   - `01_Entity%` → topic='entity'
   - `02_State%` → topic='state'
   - `03_Local%` → topic='local'
   - `04_Industry%` → topic='industry'
   - `05_Environmental%` → topic='environmental'
   - `06_Renewal%` → topic='renewal'
   - `07_Special%` → topic='special'

2. For `04_Industry_Requirements/` chunks, also extract industry_category from subdirectory:
   - `04_Industry_Requirements/Alcohol/%` → industry_category='alcohol'
   - `04_Industry_Requirements/Cannabis/%` → industry_category='cannabis'
   - `04_Industry_Requirements/Construction/%` → industry_category='construction'
   - `04_Industry_Requirements/Food_Service/%` → industry_category='food_service'
   - `04_Industry_Requirements/Healthcare/%` → industry_category='healthcare'
   - `04_Industry_Requirements/Manufacturing/%` → industry_category='manufacturing'
   - `04_Industry_Requirements/Professional_Services/%` → industry_category='professional_services'
   - `04_Industry_Requirements/Retail/%` → industry_category='retail'

3. Uncategorized files — apply manual mapping:
   - `BizInterviews%` → topic='entity'
   - `ca_business_licensing_entities%` → topic='entity'
   - `CA_DCA%` → topic='state'
   - `CA_DIR%` → topic='state'
   - `CA_FTB%` → topic='state'
   - `california-edd%` → topic='state'
   - `Initial Assessment/%` → topic='reference'
   - `research-protocol/%` → topic='reference'
   - `url_fixes%` → topic='reference'
   - `CA_Business_Licensing_URLs%` → topic='reference'
   - `README.md` → topic='reference'

Use `metadata || jsonb_build_object('topic', ...)` syntax to merge into existing JSONB without overwriting existing keys.

Do NOT use a single giant UPDATE — run one UPDATE per topic category for clarity and easier debugging. Wrap in a transaction (BEGIN/COMMIT).

Save the enrichment SQL to `BizBot_v4/database/06_enrich_metadata.sql` in the dev repo.
  </action>
  <verify>
ssh vps 'docker exec supabase-db psql -U postgres -d postgres -c "SELECT metadata->>'"'"'topic'"'"' as topic, COUNT(*) FROM bizbot_documents GROUP BY 1 ORDER BY 2 DESC;"'

Confirm: all 387 chunks have a topic assigned (no NULLs)

ssh vps 'docker exec supabase-db psql -U postgres -d postgres -c "SELECT metadata->>'"'"'industry_category'"'"' as cat, COUNT(*) FROM bizbot_documents WHERE metadata->>'"'"'topic'"'"' = '"'"'industry'"'"' GROUP BY 1 ORDER BY 1;"'

Confirm: industry chunks have industry_category populated
  </verify>
  <done>All 387 chunks have topic in metadata JSONB. Industry chunks also have industry_category. Zero NULLs on topic. Enrichment SQL saved to dev repo.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] bizbot_documents has 6 columns (id, content, metadata, embedding, created_at, updated_at)
- [ ] All 387 rows have created_at and updated_at populated
- [ ] UPDATE trigger fires (test with a dummy update + revert)
- [ ] All 387 rows have metadata->>'topic' populated
- [ ] Industry chunks have metadata->>'industry_category' populated
- [ ] No existing metadata keys were overwritten (source_file, section_heading, chunk_index, content_hash still present)
- [ ] Migration SQL files saved to dev repo
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No data loss or corruption in existing chunks
- Enrichment coverage: 100% topic, ~38% industry_category (the 147 industry chunks)
- Timestamp columns functional with auto-update trigger
  </success_criteria>

<output>
After completion, create `.planning/phases/08-rag-pipeline-improvements/08-01-SUMMARY.md`
</output>
